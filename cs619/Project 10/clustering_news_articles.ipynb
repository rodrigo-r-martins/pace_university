{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bita1828b5594684926a6374e23c22bc525",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nProgram used to clustering news articles to find trends and patterns in the data.\\nWe are using data from reddit API.\\n'"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "'''\n",
    "Program used to clustering news articles to find trends and patterns in the data.\n",
    "We are using data from reddit API.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying reddit application\n",
    "CLIENT_ID = \"A8sawF4EfvDBEQ\"\n",
    "CLIENT_SECRET = \"bvCCNdnlMNtUvn_6qLznm4uonHw\"\n",
    "USER_AGENT = \"python: <rmartinsdev-data_mining> (by /u/rmartinsdev)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating username and password\n",
    "from getpass import getpass\n",
    "\n",
    "USERNAME = \"rmartinsdev\"\n",
    "PASSWORD = \"##########\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to log with this information\n",
    "import requests\n",
    "\n",
    "def login(username, password):\n",
    "    if password is None:\n",
    "        password = getpass.getpass(f\"Enter reddit password for user {username}: \")\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    # Setup an auth object with our credentials\n",
    "    client_auth = requests.auth.HTTPBasicAuth(CLIENT_ID, CLIENT_SECRET)\n",
    "    # Make a post request to the access_token endpoint\n",
    "    post_data = {\"grant_type\": \"password\", \"username\": username, \"password\": password}\n",
    "    response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'access_token': '337851273076-Lo6gy6qsTV-CvHwjb4EjM5DZxSw', 'token_type': 'bearer', 'expires_in': 3600, 'scope': '*'}\n"
    }
   ],
   "source": [
    "# Calling the function to get an access token\n",
    "token = login(USERNAME, PASSWORD)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining sets of links from a subreddit\n",
    "subreddit = \"worldnews\"\n",
    "url = f\"https://oauth.reddit.com/r/{subreddit}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the headers\n",
    "headers = {\"Authorization\": f\"bearer {token['access_token']}\", \"User-Agent\": USER_AGENT} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requesting library to make the call, ensuring that we set the headers\n",
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Livethread X: Global COVID-19 Pandemic\nThe UK rejects Trump's offer to help with Boris Johnson's coronavirus treatment\nUK thanks Cuba for 'great gesture of solidarity' in rescuing passengers from coronavirus cruise ship - A spokesperson for cruise operator Fred Olsen said at the time that \"no other Caribbean ports were willing to accept the ship because of local sensitivities towards COVID-19 coronavirus\".\nA zoo has been trying to get two pandas to mate for 10 years. When coronavirus shut the zoo down, the pandas finally did\nBill Gates calls Taiwan's handling of coronavirus 'exemplary'\nZoom banned by Taiwan's government over China security fears\nA town mayor has been expelled from the Labour party for saying Boris Johnson ‘completely deserves’ to have coronavirus.\nSweden is facing increasing pressure to impose a coronavirus lockdown after the number of deaths rose by nearly 20 per cent in a day to 477. The government has so far resisted calls to shut down pubs, restaurants, offices and schools, while gatherings of up to 50 people are still permitted.\nAn Ontario nursing home besieged by COVID-19 didn't separate healthy from sick residents or staff until after 16 people had died, and two weeks after the home declared a respiratory outbreak, CBC News has learned.\nThe two-finger test continues to traumatise rape survivors in Pakistan; To this day, sexual assault victims are subjected to the medically unreliable—and unnecessary— virginity test\nCoronavirus: Doctors and nurses will need PTSD treatment after Covid-19 virus peaks in hospitals, warn health leaders\nSwedish hospitals have stopped using chloroquine to Treat COVID-19 after reports of Severe Side Effects.\nBoris Johnson Awake And Breathing Without Ventilator In Intensive Care, Says No.10\nChina outraged after Brazil minister suggests Covid-19 is part of 'plan for world domination'\nSpain to launch universal basic income 'soon' | While the immediate goal of the program will be to provide relief to families that have lost income due to virus-related lockdowns, the country is developing it with an eye toward making it a resource \"that stays forever\"\nLeading heart surgeon at the University Hospital of Wales dies of Coronavirus\nGermany’s number of Coronavirus daily recoveries exceeds new cases for the first time\nCOVID-19 lockdown: New Zealand Health Minister demoted after driving 20km to beach, breaking lockdown rules\nNot selling booze and tobacco during lockdown 'harmful to addicts'\n3M reaches deal with Trump administration allowing face-mask exports to Canada, Latin America\nCardinal George Pell appeal allowed - convictions quashed. He will walk free today.\nTaiwan Tells Agencies Not to Use Zoom on Security Grounds\nUK PM in intensive care\nParis bans daytime jogging in bid to slow spread of coronavirus\nTaiwan joins Canada in banning Zoom for government video conferencing\nIranian Health Official Calls Chinese Coronavirus Stats a ‘Bitter Joke’\n"
    }
   ],
   "source": [
    "# Storing the data in a dictionary and printing the titles of stories\n",
    "result = response.json()\n",
    "for story in result['data']['children']:\n",
    "    print(story['data']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that gets the data from reddit\n",
    "from time import sleep\n",
    "\n",
    "def get_links(subreddit, token, n_pages=5):\n",
    "    stories = []\n",
    "    after = None\n",
    "    for page_number in range(n_pages):\n",
    "        # Sleep before making calls to avoid going over the API limit\n",
    "        sleep(2)\n",
    "        # Setup headers and make call, just like in the login function\n",
    "        headers = {\"Authorization\": f\"bearer {token['access_token']}\", \"User-Agent\": USER_AGENT}\n",
    "        url = f\"https://oauth.reddit.com/r/{subreddit}?limit=100\"\n",
    "        if after:\n",
    "            # Append cursor for next page, if we have one\n",
    "            url += f\"&after={after}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        result = response.json()\n",
    "        # Get the new cursor for the next loop\n",
    "        after = result['data']['after']\n",
    "        # Add all os the news items to our stories list\n",
    "        for story in result['data']['children']:\n",
    "            stories.append((story['data']['title'], story['data']['url'], story['data']['score']))\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "501\n"
    }
   ],
   "source": [
    "# Calling the stories function\n",
    "stories = get_links(\"worldnews\", token)\n",
    "print(len(stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the data folder\n",
    "import os \n",
    "\n",
    "data_folder = os.path.join(os.path.expanduser(\"~\"), \"data\", \"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing hashlib library in order to use md5 hashing to create unique filenames for our articles by hashing the URL\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n0\n0\n0\n'charmap' codec can't encode character '\\u6f22' in position 8427: character maps to <undefined>\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n'charmap' codec can't encode character '\\x80' in position 62860: character maps to <undefined>\n2\n2\n2\n2\n'charmap' codec can't encode character '\\u2028' in position 213307: character maps to <undefined>\n3\n3\n3\n'charmap' codec can't encode characters in position 37364-37365: character maps to <undefined>\n4\n4\n'charmap' codec can't encode characters in position 51619-51622: character maps to <undefined>\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5\n'charmap' codec can't encode character '\\u011f' in position 1709: character maps to <undefined>\n6\n6\n6\n6\n6\n'charmap' codec can't encode character '\\u2032' in position 11235: character maps to <undefined>\n7\n7\n7\n7\n'charmap' codec can't encode characters in position 301-304: character maps to <undefined>\n8\n'charmap' codec can't encode character '\\ufeff' in position 101125: character maps to <undefined>\n9\n'charmap' codec can't encode character '\\u2028' in position 192935: character maps to <undefined>\n10\n10\n10\n'charmap' codec can't encode characters in position 54608-54609: character maps to <undefined>\n11\n'charmap' codec can't encode character '\\u200b' in position 28194: character maps to <undefined>\n12\n12\n12\n12\n12\n12\n'charmap' codec can't encode character '\\x80' in position 72894: character maps to <undefined>\n13\n'charmap' codec can't encode characters in position 166172-166173: character maps to <undefined>\n14\n14\n14\n14\n14\n14\n14\n14\n'charmap' codec can't encode character '\\u03bc' in position 716046: character maps to <undefined>\n15\n15\n'charmap' codec can't encode character '\\u2192' in position 65035: character maps to <undefined>\n16\n16\n16\n16\n'charmap' codec can't encode character '\\u0412' in position 34296: character maps to <undefined>\n17\n'charmap' codec can't encode characters in position 66320-66321: character maps to <undefined>\n18\n18\n18\n18\n18\n'charmap' codec can't encode characters in position 81992-81996: character maps to <undefined>\n19\n19\n'charmap' codec can't encode character '\\u0639' in position 10034: character maps to <undefined>\n20\n20\n20\n20\n'charmap' codec can't encode character '\\uf003' in position 63026: character maps to <undefined>\n21\n21\n21\n21\n21\n21\n21\n'charmap' codec can't encode character '\\u20b9' in position 177293: character maps to <undefined>\n22\n22\n22\n22\n22\n'charmap' codec can't encode characters in position 62098-62099: character maps to <undefined>\n23\n23\n23\n'charmap' codec can't encode character '\\u02bc' in position 416: character maps to <undefined>\n24\n24\n24\n24\n24\n24\n24\n24\n24\n'charmap' codec can't encode character '\\u2060' in position 564800: character maps to <undefined>\n25\n25\n'charmap' codec can't encode characters in position 8545-8548: character maps to <undefined>\n26\n'charmap' codec can't encode character '\\u0639' in position 10054: character maps to <undefined>\n27\n27\n'charmap' codec can't encode characters in position 2593-2599: character maps to <undefined>\n28\n'charmap' codec can't encode character '\\uf102' in position 42325: character maps to <undefined>\n29\n29\n'charmap' codec can't encode character '\\ufeff' in position 383680: character maps to <undefined>\n30\n30\n30\n30\n'charmap' codec can't encode character '\\ufffd' in position 365959: character maps to <undefined>\n31\n31\n'charmap' codec can't encode characters in position 3126-3130: character maps to <undefined>\n32\n32\n'charmap' codec can't encode character '\\uf102' in position 42148: character maps to <undefined>\n33\n'charmap' codec can't encode characters in position 94211-94224: character maps to <undefined>\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n34\n'charmap' codec can't encode characters in position 142569-142570: character maps to <undefined>\n35\n'charmap' codec can't encode character '\\u200b' in position 26902: character maps to <undefined>\n36\n36\n'charmap' codec can't encode characters in position 18474-18476: character maps to <undefined>\n37\n'charmap' codec can't encode characters in position 38174-38175: character maps to <undefined>\n38\n38\n38\n38\n'charmap' codec can't encode character '\\u26a1' in position 23: character maps to <undefined>\n39\n39\n'charmap' codec can't encode character '\\u306f' in position 29229: character maps to <undefined>\n40\n40\n40\n40\n40\n'charmap' codec can't encode characters in position 43507-43511: character maps to <undefined>\n41\n41\n'charmap' codec can't encode characters in position 15279-15282: character maps to <undefined>\n42\n42\n42\n42\n'charmap' codec can't encode characters in position 88589-88593: character maps to <undefined>\n43\n43\n43\n'charmap' codec can't encode characters in position 59948-59953: character maps to <undefined>\n44\n44\n44\n44\n'charmap' codec can't encode character '\\uf102' in position 41903: character maps to <undefined>\n45\n45\n45\n'charmap' codec can't encode characters in position 165543-165544: character maps to <undefined>\n46\n46\n'charmap' codec can't encode character '\\u26a1' in position 428: character maps to <undefined>\n47\n47\n47\n47\n47\n47\n47\n'charmap' codec can't encode character '\\u010c' in position 49688: character maps to <undefined>\n48\n'charmap' codec can't encode character '\\u0141' in position 455981: character maps to <undefined>\n49\n49\n49\n'charmap' codec can't encode character '\\u20b9' in position 176330: character maps to <undefined>\n50\n'charmap' codec can't encode character '\\u2032' in position 11943: character maps to <undefined>\n51\n51\n51\n'charmap' codec can't encode character '\\x80' in position 74599: character maps to <undefined>\n52\n52\n52\n'charmap' codec can't encode character '\\ufeff' in position 37827: character maps to <undefined>\n53\n'charmap' codec can't encode character '\\x80' in position 75326: character maps to <undefined>\n54\n'charmap' codec can't encode character '\\u2009' in position 2093: character maps to <undefined>\n55\n55\n'charmap' codec can't encode character '\\uf102' in position 41632: character maps to <undefined>\n56\n56\n56\n56\n56\n'charmap' codec can't encode characters in position 305-310: character maps to <undefined>\n57\n57\n'charmap' codec can't encode character '\\uf102' in position 43209: character maps to <undefined>\n58\n58\n'charmap' codec can't encode character '\\u2032' in position 11176: character maps to <undefined>\n59\n59\n'charmap' codec can't encode characters in position 62818-62819: character maps to <undefined>\n60\n60\n60\n'charmap' codec can't encode character '\\u26a1' in position 23: character maps to <undefined>\n61\nExceeded 30 redirects.\n62\n62\n'charmap' codec can't encode character '\\u011f' in position 718855: character maps to <undefined>\n63\n'charmap' codec can't encode characters in position 20555-20561: character maps to <undefined>\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n64\n'charmap' codec can't encode character '\\u0141' in position 457396: character maps to <undefined>\n65\n65\n65\n65\n65\n'charmap' codec can't encode characters in position 42543-42549: character maps to <undefined>\n66\n'charmap' codec can't encode characters in position 14222-14228: character maps to <undefined>\n67\n67\n67\n67\n67\n67\n'charmap' codec can't encode character '\\u017a' in position 46640: character maps to <undefined>\n68\n68\n68\n68\n68\n'charmap' codec can't encode character '\\u011f' in position 930: character maps to <undefined>\n69\n'charmap' codec can't encode character '\\u011f' in position 736107: character maps to <undefined>\n70\n70\n'charmap' codec can't encode character '\\u20b1' in position 50002: character maps to <undefined>\n71\n71\n'charmap' codec can't encode characters in position 19090-19092: character maps to <undefined>\n72\n72\n72\n'charmap' codec can't encode character '\\u02bc' in position 416: character maps to <undefined>\n73\n73\n73\n73\n73\n'charmap' codec can't encode character '\\uf1e0' in position 52746: character maps to <undefined>\n74\n74\n74\n'charmap' codec can't encode character '\\u200b' in position 278461: character maps to <undefined>\n75\n75\n75\n75\n75\n75\n75\n75\n75\n75\n'charmap' codec can't encode characters in position 50044-50047: character maps to <undefined>\n76\n'charmap' codec can't encode characters in position 8464-8465: character maps to <undefined>\n77\n'charmap' codec can't encode characters in position 2561-2567: character maps to <undefined>\n78\n78\n78\n'charmap' codec can't encode characters in position 31272-31273: character maps to <undefined>\n79\n'charmap' codec can't encode characters in position 62532-62537: character maps to <undefined>\n80\n'charmap' codec can't encode character '\\u2028' in position 64075: character maps to <undefined>\n81\n'charmap' codec can't encode character '\\u25bc' in position 98748: character maps to <undefined>\n82\n82\n82\n'charmap' codec can't encode characters in position 10909-10912: character maps to <undefined>\n83\n'charmap' codec can't encode characters in position 140179-140184: character maps to <undefined>\n84\n84\n84\n84\n84\n84\n'charmap' codec can't encode character '\\U0001f388' in position 316698: character maps to <undefined>\n85\n'charmap' codec can't encode character '\\u306f' in position 28899: character maps to <undefined>\n86\n86\n86\n86\n86\n'charmap' codec can't encode character '\\u2715' in position 103550: character maps to <undefined>\n87\n87\n87\n'charmap' codec can't encode character '\\u3000' in position 537: character maps to <undefined>\n88\n88\n88\n88\n88\n'charmap' codec can't encode characters in position 10739-10742: character maps to <undefined>\n89\n'charmap' codec can't encode character '\\u2033' in position 35843: character maps to <undefined>\n90\n90\n90\n90\n90\n'charmap' codec can't encode character '\\u011f' in position 718855: character maps to <undefined>\n91\n91\n91\n91\n91\n'charmap' codec can't encode character '\\u05de' in position 265140: character maps to <undefined>\n92\n'charmap' codec can't encode character '\\u200b' in position 45016: character maps to <undefined>\n93\n93\n'charmap' codec can't encode characters in position 24576-24577: character maps to <undefined>\n94\n'charmap' codec can't encode character '\\u20b9' in position 113645: character maps to <undefined>\n95\n95\n'charmap' codec can't encode character '\\x8d' in position 23819: character maps to <undefined>\n96\n'charmap' codec can't encode characters in position 43507-43511: character maps to <undefined>\n97\n97\n97\n'charmap' codec can't encode character '\\u20b9' in position 149148: character maps to <undefined>\n98\n'charmap' codec can't encode character '\\u2028' in position 193202: character maps to <undefined>\n99\n99\n'charmap' codec can't encode characters in position 1598-1599: character maps to <undefined>\n100\n"
    }
   ],
   "source": [
    "# Iterating through each story, downloading website and saving the results to a file\n",
    "number_errors = 0\n",
    "\n",
    "for title, url, score in stories:\n",
    "    if number_errors == 100:\n",
    "        break\n",
    "    output_filename = hashlib.md5(url.encode()).hexdigest() \n",
    "    fullpath = os.path.join(data_folder, output_filename + \".txt\")\n",
    "    try: \n",
    "        response = requests.get(url) \n",
    "        data = response.text \n",
    "        with open(fullpath, 'w') as outf: \n",
    "            outf.write(data)\n",
    "    except Exception as e:\n",
    "        number_errors += 1\n",
    "        print(e)\n",
    "        # You can use this to view the errors, if you are getting too many:\n",
    "        # raise\n",
    "        print(number_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of errors: 100\n"
    }
   ],
   "source": [
    "# Checking the number of errors obtained\n",
    "print(f\"Number of errors: {number_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Getting a list of each of the filenames in our raw subfolder\n",
    "filenames = [os.path.join(data_folder, filename) for filename in os.listdir(data_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the data folder for the text-only versions that we will extract\n",
    "text_output_folder = os.path.join(os.path.expanduser(\"~\"), \"data\", \"textonly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code that will extract the text from files\n",
    "from lxml import etree\n",
    "\n",
    "skip_node_types = [\"script\", \"head\", \"style\", etree.Comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that parses an HTML file into an lxml etree\n",
    "parser = etree.HTMLParser()\n",
    "\n",
    "def get_text_from_file(filename):\n",
    "    with open(filename) as inf:\n",
    "        html_tree = etree.parse(inf, parser)\n",
    "    return get_text_from_node(html_tree.getroot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that checks if the text is at least 100 characters long\n",
    "def get_text_from_node(node):\n",
    "    try:\n",
    "        if len(node) == 0: \n",
    "            # No children, just return text from this item\n",
    "            if node.text:\n",
    "                return node.text \n",
    "            else:\n",
    "                return \"\"\n",
    "        else:\n",
    "            # This node has children, return the text from it:\n",
    "            results = (get_text_from_node(child)\n",
    "                    for child in node\n",
    "                    if child.tag not in skip_node_types)\n",
    "        result = str.join(\"\\n\", (r for r in results if len(r) > 1))\n",
    "        if len(result) >= 100:\n",
    "            return result\n",
    "        else:\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through the raw HTML pages and saving results to the textonly subfolder\n",
    "for filename in os.listdir(data_folder):\n",
    "    try:\n",
    "        text = get_text_from_file(os.path.join(data_folder, filename)) \n",
    "        with open(os.path.join(text_output_folder, filename), 'w') as outf: \n",
    "            outf.write(text)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing k-means algorithm and CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline for the analysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "n_clusters = 10\n",
    "pipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4)), ('clusterer', KMeans(n_clusters=n_clusters))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and predicting the pipeline\n",
    "documents = [open(os.path.join(text_output_folder, filename)).read()\n",
    "                for filename in os.listdir(text_output_folder)]\n",
    "pipeline.fit(documents)\n",
    "labels = pipeline.predict(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Cluster 0 contains 257 samples\nCluster 1 contains 13 samples\nCluster 2 contains 1 samples\nCluster 3 contains 1 samples\nCluster 4 contains 5 samples\nCluster 5 contains 6 samples\nCluster 6 contains 3 samples\nCluster 7 contains 30 samples\nCluster 8 contains 2 samples\nCluster 9 contains 4 samples\n"
    }
   ],
   "source": [
    "# Checking the number of samples in each cluster\n",
    "from collections import Counter\n",
    "\n",
    "c = Counter(labels)\n",
    "for cluster_number in range(n_clusters):\n",
    "    print(f\"Cluster {cluster_number} contains {c[cluster_number]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "146.30843263452965"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "# Checking algorithm result\n",
    "pipeline.named_steps['clusterer'].inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a list of scores for each n_cluster value between 2 and 20\n",
    "inertia_scores = []\n",
    "n_cluster_values = list(range(2, 20))\n",
    "for n_clusters in n_cluster_values:\n",
    "    cur_inertia_scores = []\n",
    "    X = TfidfVectorizer(max_df=0.4).fit_transform(documents)\n",
    "    for i in range(10):\n",
    "        km = KMeans(n_clusters=n_clusters).fit(X)\n",
    "        cur_inertia_scores.append(km.inertia_)\n",
    "        inertia_scores.append(cur_inertia_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerunning algorithm according to the plot result\n",
    "n_clusters = 6\n",
    "pipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4)), ('clusterer', KMeans(n_clusters=n_clusters))])\n",
    "pipeline.fit(documents)\n",
    "labels = pipeline.predict(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the term list from our feature extraction step\n",
    "terms = pipeline.named_steps['feature_extraction'].get_feature_names()\n",
    "c = Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Cluster 0 contains 268 samples\n  Most important terms\n  1) he (score: 0.0238)\n  2) coronavirus (score: 0.0209)\n  3) was (score: 0.0184)\n  4) will (score: 0.0176)\n  5) they (score: 0.0175)\nCluster 1 contains 1 samples\n  Most important terms\n  1) mr (score: 0.3122)\n  2) government (score: 0.2082)\n  3) he (score: 0.2017)\n  4) care (score: 0.1731)\n  5) decisions (score: 0.1707)\nCluster 2 contains 13 samples\n  Most important terms\n  1) cache (score: 0.4414)\n  2) found (score: 0.3217)\n  3) 404 (score: 0.2207)\n  4) server (score: 0.2207)\n  5) mediation (score: 0.2207)\nCluster 3 contains 5 samples\n  Most important terms\n  1) detected (score: 0.2977)\n  2) robot (score: 0.2977)\n  3) unusual (score: 0.2895)\n  4) computer (score: 0.2895)\n  5) click (score: 0.2822)\nCluster 4 contains 32 samples\n  Most important terms\n  1) tablet (score: 0.2705)\n  2) browser (score: 0.2030)\n  3) landscape (score: 0.1503)\n  4) medium (score: 0.1476)\n  5) portrait (score: 0.1202)\nCluster 5 contains 3 samples\n  Most important terms\n  1) zoom (score: 0.5211)\n  2) taiwan (score: 0.3191)\n  3) video (score: 0.1263)\n  4) microsoft (score: 0.1222)\n  5) google (score: 0.1111)\n"
    }
   ],
   "source": [
    "# Iterate over each cluster and print the size of cluster and most important terms for this cluster\n",
    "for cluster_number in range(n_clusters):\n",
    "    print(f\"Cluster {cluster_number} contains {c[cluster_number]} samples\")\n",
    "    print(f\"  Most important terms\")\n",
    "    centroid = pipeline.named_steps['clusterer'].cluster_centers_[cluster_number]\n",
    "    most_important = centroid.argsort()\n",
    "    for i in range(5):\n",
    "        term_index = most_important[-(i+1)]\n",
    "        print(f\"  {i+1}) {terms[term_index]} (score: {centroid[term_index]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline(memory=None,\n         steps=[('feature_extraction',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=True, max_df=0.4, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=['a', 'able', 'aboard', 'about',\n                                             'above', 'absent'...\n                                             'and', 'another', 'anti', 'any',\n                                             'anybody', 'anyone', ...],\n                                 strip_accents=None, sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, use_idf=True,\n                                 vocabulary=None)),\n                ('clusterer',\n                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\n                        max_iter=300, n_clusters=6, n_init=10, n_jobs=None,\n                        precompute_distances='auto', random_state=None,\n                        tol=0.0001, verbose=0))],\n         verbose=False)"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "# Ignoring some words\n",
    "function_words = [\"a\", \"able\", \"aboard\", \"about\", \"above\", \"absent\",\n",
    "                \"according\" , \"accordingly\", \"across\", \"after\", \"against\",\n",
    "                \"ahead\", \"albeit\", \"all\", \"along\", \"alongside\", \"although\",\n",
    "                \"am\", \"amid\", \"amidst\", \"among\", \"amongst\", \"amount\", \"an\",\n",
    "                \"and\", \"another\", \"anti\", \"any\", \"anybody\", \"anyone\",\n",
    "                \"anything\", \"are\", \"around\", \"as\", \"aside\", \"astraddle\",\n",
    "                \"astride\", \"at\", \"away\", \"bar\", \"barring\", \"be\", \"because\",\n",
    "                \"been\", \"before\", \"behind\", \"being\", \"below\", \"beneath\",\n",
    "                \"beside\", \"besides\", \"better\", \"between\", \"beyond\", \"bit\",\n",
    "                \"both\", \"but\", \"by\", \"can\", \"certain\", \"circa\", \"close\",\n",
    "                \"concerning\", \"consequently\", \"considering\", \"could\",\n",
    "                \"couple\", \"dare\", \"deal\", \"despite\", \"down\", \"due\", \"during\",\n",
    "                \"each\", \"eight\", \"eighth\", \"either\", \"enough\", \"every\",\n",
    "                \"everybody\", \"everyone\", \"everything\", \"except\", \"excepting\",\n",
    "                \"excluding\", \"failing\", \"few\", \"fewer\", \"fifth\", \"first\",\n",
    "                \"five\", \"following\", \"for\", \"four\", \"fourth\", \"from\", \"front\",\n",
    "                \"given\", \"good\", \"great\", \"had\", \"half\", \"have\", \"he\",\n",
    "                \"heaps\", \"hence\", \"her\", \"hers\", \"herself\", \"him\", \"himself\",\n",
    "                \"his\", \"however\", \"i\", \"if\", \"in\", \"including\", \"inside\",\n",
    "                \"instead\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keeping\",\n",
    "                \"lack\", \"less\", \"like\", \"little\", \"loads\", \"lots\", \"majority\",\n",
    "                \"many\", \"masses\", \"may\", \"me\", \"might\", \"mine\", \"minority\",\n",
    "                \"minus\", \"more\", \"most\", \"much\", \"must\", \"my\", \"myself\",\n",
    "                \"near\", \"need\", \"neither\", \"nevertheless\", \"next\", \"nine\",\n",
    "                \"ninth\", \"no\", \"nobody\", \"none\", \"nor\", \"nothing\",\n",
    "                \"notwithstanding\", \"number\", \"numbers\", \"of\", \"off\", \"on\",\n",
    "                \"once\", \"one\", \"onto\", \"opposite\", \"or\", \"other\", \"ought\",\n",
    "                \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"part\",\n",
    "                \"past\", \"pending\", \"per\", \"pertaining\", \"place\", \"plenty\",\n",
    "                \"plethora\", \"plus\", \"quantities\", \"quantity\", \"quarter\",\n",
    "                \"regarding\", \"remainder\", \"respecting\", \"rest\", \"round\",\n",
    "                \"save\", \"saving\", \"second\", \"seven\", \"seventh\", \"several\",\n",
    "                \"shall\", \"she\", \"should\", \"similar\", \"since\", \"six\", \"sixth\",\n",
    "                \"so\", \"some\", \"somebody\", \"someone\", \"something\", \"spite\",\n",
    "                \"such\", \"ten\", \"tenth\", \"than\", \"thanks\", \"that\", \"the\",\n",
    "                \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\",\n",
    "                \"therefore\", \"these\", \"they\", \"third\", \"this\", \"those\",\n",
    "                \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\",\n",
    "                \"till\", \"time\", \"to\", \"tons\", \"top\", \"toward\", \"towards\",\n",
    "                \"two\", \"under\", \"underneath\", \"unless\", \"unlike\", \"until\",\n",
    "                \"unto\", \"up\", \"upon\", \"us\", \"used\", \"various\", \"versus\",\n",
    "                \"via\", \"view\", \"wanting\", \"was\", \"we\", \"were\", \"what\",\n",
    "                \"whatever\", \"when\", \"whenever\", \"where\", \"whereas\",\n",
    "                \"wherever\", \"whether\", \"which\", \"whichever\", \"while\",\n",
    "                \"whilst\", \"who\", \"whoever\", \"whole\", \"whom\", \"whomever\",\n",
    "                \"whose\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\",\n",
    "                \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
    "\n",
    "pipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4, stop_words=function_words)), ('clusterer', KMeans(n_clusters=n_clusters))])\n",
    "pipeline.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset with a lower number of features by taking the distance to each centroid as a feature\n",
    "X = pipeline.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a co-association matrix from an array of labels\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def create_coassociation_matrix(labels):\n",
    "    rows = []\n",
    "    cols = []\n",
    "    unique_labels = set(labels)\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)[0]\n",
    "        for index1 in indices:\n",
    "            for index2 in indices:\n",
    "                rows.append(index1)\n",
    "                cols.append(index2)\n",
    "    data = np.ones((len(rows),))\n",
    "    return csr_matrix((data, (rows, cols)), dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(0, 0)\t1.0\n  (0, 1)\t1.0\n  (0, 2)\t1.0\n  (0, 3)\t1.0\n  (0, 4)\t1.0\n  (0, 5)\t1.0\n  (0, 6)\t1.0\n  (0, 7)\t1.0\n  (0, 8)\t1.0\n  (0, 9)\t1.0\n  (0, 11)\t1.0\n  (0, 12)\t1.0\n  (0, 15)\t1.0\n  (0, 16)\t1.0\n  (0, 17)\t1.0\n  (0, 18)\t1.0\n  (0, 19)\t1.0\n  (0, 20)\t1.0\n  (0, 21)\t1.0\n  (0, 22)\t1.0\n  (0, 23)\t1.0\n  (0, 24)\t1.0\n  (0, 25)\t1.0\n  (0, 27)\t1.0\n  (0, 28)\t1.0\n  :\t:\n  (320, 293)\t1.0\n  (320, 294)\t1.0\n  (320, 295)\t1.0\n  (320, 296)\t1.0\n  (320, 297)\t1.0\n  (320, 298)\t1.0\n  (320, 299)\t1.0\n  (320, 300)\t1.0\n  (320, 301)\t1.0\n  (320, 302)\t1.0\n  (320, 303)\t1.0\n  (320, 305)\t1.0\n  (320, 306)\t1.0\n  (320, 309)\t1.0\n  (320, 310)\t1.0\n  (320, 311)\t1.0\n  (320, 312)\t1.0\n  (320, 313)\t1.0\n  (320, 315)\t1.0\n  (320, 316)\t1.0\n  (320, 318)\t1.0\n  (320, 320)\t1.0\n  (321, 107)\t1.0\n  (321, 319)\t1.0\n  (321, 321)\t1.0\n"
    }
   ],
   "source": [
    "# Calling the function and getting the co-association matrix\n",
    "C = create_coassociation_matrix(labels)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the Minimum Spanning Tree\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "\n",
    "mst = minimum_spanning_tree(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mst = minimum_spanning_tree(-C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any node with a weight less than a predefined threshold\n",
    "pipeline.fit(documents)\n",
    "labels2 = pipeline.predict(documents)\n",
    "C2 = create_coassociation_matrix(labels2)\n",
    "C_sum = (C+C2)/2\n",
    "mst = minimum_spanning_tree(-C_sum)\n",
    "mst.data[mst.data > -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of clusters: 3\n"
    }
   ],
   "source": [
    "from scipy.sparse.csgraph import connected_components \n",
    "number_of_clusters, labels = connected_components(mst)\n",
    "print(f\"Number of clusters: {number_of_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClusterMixin \n",
    "class EAC(BaseEstimator, ClusterMixin):\n",
    "    def __init__(self, n_clusterings=10, cut_threshold=0.5, n_clusters_range=(3, 10)): \n",
    "        self.n_clusterings = n_clusterings\n",
    "        self.cut_threshold = cut_threshold\n",
    "        self.n_clusters_range = n_clusters_range\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        C = sum((create_coassociation_matrix(self._single_clustering(X))\n",
    "                 for i in range(self.n_clusterings)))\n",
    "        mst = minimum_spanning_tree(-C)\n",
    "        mst.data[mst.data > -self.cut_threshold] = 0\n",
    "        mst.eliminate_zeros()\n",
    "        self.n_components, self.labels_ = connected_components(mst)\n",
    "        return self\n",
    " \n",
    "    def _single_clustering(self, X):\n",
    "        n_clusters = np.random.randint(*self.n_clusters_range)\n",
    "        km = KMeans(n_clusters=n_clusters)\n",
    "        return km.fit_predict(X)\n",
    " \n",
    "    def fit_predict(self, X):\n",
    "        self.fit(X)\n",
    "        return self.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4)),\n",
    "                     ('clusterer', EAC()) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(max_df=0.4) \n",
    "X = vec.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans \n",
    "mbkm = MiniBatchKMeans(random_state=14, n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10 \n",
    "for iteration in range(int(X.shape[0] / batch_size)): \n",
    "    start = batch_size * iteration \n",
    "    end = batch_size * (iteration + 1) \n",
    "    mbkm.partial_fit(X[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2 2 2 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 1 2 2 1 1 2 2 1 2 1 2 2 2 1 2 1 2 1 1\n 2 2 1 2 2 1 2 1 2 1 2 2 1 2 1 2 1 2 2 2 1 1 2 1 2 1 1 2 2 2 1 1 1 2 1 2 2\n 1 1 1 1 1 1 2 2 2 1 1 1 1 2 2 1 2 1 2 2 1 2 2 2 2 1 1 2 2 2 1 1 1 2 1 2 1\n 2 2 2 1 2 1 2 2 2 1 1 2 1 2 2 2 2 1 2 2 2 2 2 2 2 2 1 2 1 2 2 1 2 2 2 2 2\n 2 2 1 1 2 2 1 1 1 2 1 1 1 1 2 2 1 2 1 2 2 1 2 2 1 1 2 2 2 2 2 1 2 2 2 1 1\n 1 1 2 2 2 2 2 2 2 2 1 2 1 2 1 1 2 1 2 1 1 1 1 0 2 2 2 1 1 1 2 1 1 2 2 2 2\n 1 1 2 1 1 2 2 2 1 2 1 2 2 1 2 2 2 2 2 1 1 2 2 2 1 2 2 2 1 2 2 2 2 1 1 2 1\n 1 1 1 2 2 1 2 2 1 1 1 2 1 1 2 2 1 1 2 2 1 1 1 1 2 2 2 1 1 2 2 1 2 1 1 1 2\n 2 1 2 1 1 1 2 1 2 1 1 2 2 2 2 1 1 2 2 2 2 2 1 2 1 2]\n"
    }
   ],
   "source": [
    "labels = mbkm.predict(X)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialFitPipeline(Pipeline):\n",
    "    def partial_fit(self, X, y=None):\n",
    "        Xt = X\n",
    "        for name, transform in self.steps[:-1]:\n",
    "            Xt = transform.transform(Xt)\n",
    "        return self.steps[-1][1].partial_fit(Xt, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1\n 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0\n 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1\n 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0\n 0 0 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 1\n 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 2 0 0 0 1 1 1 0 1 1 1 0 0 0\n 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1\n 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0\n 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0]\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "pipeline = PartialFitPipeline([('feature_extraction', HashingVectorizer()), ('clusterer', MiniBatchKMeans(random_state=14, n_clusters=3))])\n",
    "batch_size = 10\n",
    "for iteration in range(int(len(documents) / batch_size)): \n",
    "    start = batch_size * iteration \n",
    "    end = batch_size * (iteration + 1)\n",
    "    pipeline.partial_fit(documents[start:end]) \n",
    "labels = pipeline.predict(documents)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}