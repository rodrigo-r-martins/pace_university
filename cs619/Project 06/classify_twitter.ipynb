{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program used for performing the classification of data\n",
    "import spacy\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Create a spaCy parser\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class BagOfWords(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [{word: True for word in word_tokenize(document)}\n",
    "                 for document in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing DictVectorizer to convert the dictionaries into a matrix\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing BernoulliNB classifier to use in our dataset\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_filename = os.path.join(os.path.expanduser('~'), 'OneDrive', 'Desktop', 'Pace', 'CS619', 'Chapter06', 'Data', 'python_tweets.json')\n",
    "labels_filename = os.path.join(os.path.expanduser('~'), 'OneDrive', 'Desktop', 'Pace', 'CS619', 'Chapter06', 'Data', 'python_classes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 tweets\n"
     ]
    }
   ],
   "source": [
    "# Loading the tweets\n",
    "import json\n",
    "\n",
    "tweets = []\n",
    "with open(input_filename) as inf:\n",
    "    for line in inf:\n",
    "        if len(line.strip()) == 0: \n",
    "            continue\n",
    "        tweets.append(json.loads(line)['text'])\n",
    "print(f'Loaded {len(tweets)} tweets')\n",
    "\n",
    "with open(labels_filename) as inf:\n",
    "    labels = json.load(inf)\n",
    "    \n",
    "# Ensure only classified tweets are loaded\n",
    "# tweets = tweets[:len(labels)]\n",
    "# assert len(tweets) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = min(len(tweets), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets = [t.lower() for t in tweets[:n_samples]]\n",
    "labels = labels[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_true = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.0% have class 1\n"
     ]
    }
   ],
   "source": [
    "print(f'{np.mean(y_true == 1) * 100:.1f}% have class 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline with all components together\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('bag-of-words', BagOfWords()), ('vectorizer', DictVectorizer()), ('naive-bayes', BernoulliNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.843\n"
     ]
    }
   ],
   "source": [
    "# Applying F1-score method to the database\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "scores = cross_val_score(pipeline, sample_tweets, y_true, cv=10, scoring='f1')\n",
    "\n",
    "# We then print out the average of the scores:\n",
    "print(f'Score: {np.mean(scores):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bag-of-words',\n",
       "                 <__main__.BagOfWords object at 0x0000020561A59880>),\n",
       "                ('vectorizer',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=True)),\n",
       "                ('naive-bayes',\n",
       "                 BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None,\n",
       "                             fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a new model\n",
    "model = pipeline.fit(sample_tweets, labels)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Naive Bayes model\n",
    "nb = model.named_steps['naive-bayes']\n",
    "feature_probabilities = nb.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the array of algorithm probabilities\n",
    "top_features = np.argsort(-nb.feature_log_prob_[1][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from pipeline\n",
    "dv = model.named_steps['vectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.9444444444444443\n",
      "1 python 0.8703703703703701\n",
      "2 https 0.7777777777777778\n",
      "3 # 0.6481481481481479\n",
      "4 @ 0.6481481481481479\n",
      "5 rt 0.48148148148148145\n",
      "6 | 0.40740740740740744\n",
      "7 automated 0.40740740740740744\n",
      "8 a 0.3703703703703703\n",
      "9 , 0.35185185185185175\n",
      "10 . 0.27777777777777773\n",
      "11 to 0.2592592592592592\n",
      "12 and 0.2407407407407407\n",
      "13 hasdid 0.2222222222222222\n",
      "14 for 0.2222222222222222\n",
      "15 with 0.2222222222222222\n",
      "16 the 0.1851851851851852\n",
      "17 i 0.16666666666666669\n",
      "18 in 0.16666666666666669\n",
      "19 is 0.1481481481481481\n",
      "20 learning 0.1481481481481481\n",
      "21 this 0.12962962962962962\n",
      "22 of 0.12962962962962962\n",
      "23 it 0.12962962962962962\n",
      "24 can 0.11111111111111109\n",
      "25 using 0.11111111111111109\n",
      "26 you 0.11111111111111109\n",
      "27 on 0.11111111111111109\n",
      "28 have 0.11111111111111109\n",
      "29 about 0.11111111111111109\n",
      "30 your 0.09259259259259259\n",
      "31 deep 0.09259259259259259\n",
      "32 performance 0.09259259259259259\n",
      "33 've 0.09259259259259259\n",
      "34 learn 0.09259259259259259\n",
      "35 activation 0.07407407407407407\n",
      "36 visualize 0.07407407407407407\n",
      "37 code 0.07407407407407407\n",
      "38 analytics 0.07407407407407407\n",
      "39 start 0.07407407407407407\n",
      "40 `` 0.07407407407407407\n",
      "41 but 0.07407407407407407\n",
      "42 build 0.07407407407407407\n",
      "43 api 0.07407407407407407\n",
      "44 service 0.07407407407407407\n",
      "45 class 0.07407407407407407\n",
      "46 check 0.07407407407407407\n",
      "47 tensorflow 0.07407407407407407\n",
      "48 flask 0.07407407407407407\n",
      "49 maps 0.07407407407407407\n",
      "50 master 0.07407407407407407\n",
      "51 keras 0.07407407407407407\n",
      "52 grad-cam 0.07407407407407407\n",
      "53 will 0.07407407407407407\n",
      "54 way 0.07407407407407407\n",
      "55 ’ 0.07407407407407407\n",
      "56 great 0.07407407407407407\n",
      "57 if 0.07407407407407407\n",
      "58 '' 0.07407407407407407\n",
      "59 - 0.07407407407407407\n",
      "60 be 0.055555555555555566\n",
      "61 all 0.055555555555555566\n",
      "62 friendly 0.055555555555555566\n",
      "63 sqlalchemy 0.055555555555555566\n",
      "64 html 0.055555555555555566\n",
      "65 liked 0.055555555555555566\n",
      "66 sum 0.055555555555555566\n",
      "67 apm 0.055555555555555566\n",
      "68 images 0.055555555555555566\n",
      "69 into 0.055555555555555566\n",
      "70 javascript 0.055555555555555566\n",
      "71 any 0.055555555555555566\n",
      "72 app 0.055555555555555566\n",
      "73 site 0.055555555555555566\n",
      "74 developers 0.055555555555555566\n",
      "75 lists 0.055555555555555566\n",
      "76 now 0.055555555555555566\n",
      "77 datadog 0.055555555555555566\n",
      "78 percentage 0.055555555555555566\n",
      "79 notification 0.055555555555555566\n",
      "80 plotting 0.055555555555555566\n",
      "81 django 0.055555555555555566\n",
      "82 convert 0.055555555555555566\n",
      "83 constraints 0.055555555555555566\n",
      "84 post-mortem 0.055555555555555566\n",
      "85 shipment 0.055555555555555566\n",
      "86 programming 0.055555555555555566\n",
      "87 pytesseract 0.055555555555555566\n",
      "88 no 0.055555555555555566\n",
      "89 easypost 0.055555555555555566\n",
      "90 my 0.055555555555555566\n",
      "91 read 0.055555555555555566\n",
      "92 relationships 0.055555555555555566\n",
      "93 fields 0.055555555555555566\n",
      "94 s 0.055555555555555566\n",
      "95 made 0.055555555555555566\n",
      "96 not 0.055555555555555566\n",
      "97 optimizing 0.055555555555555566\n",
      "98 ! 0.055555555555555566\n",
      "99 //t.co/5wxfmynfnn 0.055555555555555566\n",
      "100 //t.co/he8l4502y5 0.055555555555555566\n",
      "101 //t.co/ctxpchm2da 0.055555555555555566\n",
      "102 //t.co/w3qehn1sx7 0.055555555555555566\n",
      "103 //t.co/2rzz3otf54 0.055555555555555566\n",
      "104 time 0.055555555555555566\n",
      "105 youtube 0.055555555555555566\n",
      "106 ; 0.055555555555555566\n",
      "107 use 0.055555555555555566\n",
      "108 //t.co/tbqpnybnhd 0.055555555555555566\n",
      "109 ? 0.055555555555555566\n",
      "110 twilio 0.055555555555555566\n",
      "111 zillow 0.055555555555555566\n",
      "112 //t.co/c6aojalh… 0.055555555555555566\n",
      "113 //t.co/rkjy7mzkbh 0.055555555555555566\n",
      "114 101 0.055555555555555566\n",
      "115 //t.co/ybupfq0do0 0.055555555555555566\n",
      "116 ai 0.055555555555555566\n",
      "117 //t.co/mqqwp0sd41 0.055555555555555566\n",
      "118 & 0.055555555555555566\n",
      "119 myself 0.037037037037037035\n",
      "120 wip 0.037037037037037035\n",
      "121 maybe 0.037037037037037035\n",
      "122 mulloym 0.037037037037037035\n",
      "123 me 0.037037037037037035\n",
      "124 muito 0.037037037037037035\n",
      "125 mongodb 0.037037037037037035\n",
      "126 much 0.037037037037037035\n",
      "127 mo_yosola 0.037037037037037035\n",
      "128 motion 0.037037037037037035\n",
      "129 n't 0.037037037037037035\n",
      "130 more 0.037037037037037035\n",
      "131 muita 0.037037037037037035\n",
      "132 meu 0.037037037037037035\n",
      "133 thanks 0.037037037037037035\n",
      "134 windows 0.037037037037037035\n",
      "135 want 0.037037037037037035\n",
      "136 options 0.037037037037037035\n",
      "137 was 0.037037037037037035\n",
      "138 optimize 0.037037037037037035\n",
      "139 one 0.037037037037037035\n",
      "140 website 0.037037037037037035\n",
      "141 ohhhh 0.037037037037037035\n",
      "142 occurred 0.037037037037037035\n",
      "143 obtener 0.037037037037037035\n",
      "144 weekend 0.037037037037037035\n",
      "145 objetivo 0.037037037037037035\n",
      "146 whales 0.037037037037037035\n",
      "147 which 0.037037037037037035\n",
      "148 whitespace 0.037037037037037035\n",
      "149 notebook 0.037037037037037035\n",
      "150 why 0.037037037037037035\n",
      "151 materials 0.037037037037037035\n",
      "152 noob… 0.037037037037037035\n",
      "153 node 0.037037037037037035\n",
      "154 new 0.037037037037037035\n",
      "155 work 0.037037037037037035\n",
      "156 maintainable 0.037037037037037035\n",
      "157 march 0.037037037037037035\n",
      "158 justmarkham 0.037037037037037035\n",
      "159 just 0.037037037037037035\n",
      "160 joseojedarojas 0.037037037037037035\n",
      "161 jonathanjsdev 0.037037037037037035\n",
      "162 jodielord5 0.037037037037037035\n",
      "163 jennchem 0.037037037037037035\n",
      "164 its 0.037037037037037035\n",
      "165 é 0.037037037037037035\n",
      "166 “ 0.037037037037037035\n",
      "167 ” 0.037037037037037035\n",
      "168 ☞ 0.037037037037037035\n",
      "169 iot 0.037037037037037035\n",
      "170 🎉🎉 0.037037037037037035\n",
      "171 installed 0.037037037037037035\n",
      "172 indicar 0.037037037037037035\n",
      "173 🐼🤹‍♂️ 0.037037037037037035\n",
      "174 👈… 0.037037037037037035\n",
      "175 idea 0.037037037037037035\n",
      "176 👉 0.037037037037037035\n",
      "177 keep 0.037037037037037035\n",
      "178 knc402 0.037037037037037035\n",
      "179 labs 0.037037037037037035\n",
      "180 yesterday 0.037037037037037035\n",
      "181 working 0.037037037037037035\n",
      "182 workshops 0.037037037037037035\n",
      "183 making 0.037037037037037035\n",
      "184 makerdemy 0.037037037037037035\n",
      "185 or 0.037037037037037035\n",
      "186 would 0.037037037037037035\n",
      "187 write 0.037037037037037035\n",
      "188 machinelearning… 0.037037037037037035\n",
      "189 machinelearning 0.037037037037037035\n",
      "190 mas 0.037037037037037035\n",
      "191 writing 0.037037037037037035\n",
      "192 love 0.037037037037037035\n",
      "193 ll 0.037037037037037035\n",
      "194 little 0.037037037037037035\n",
      "195 w… 0.037037037037037035\n",
      "196 list 0.037037037037037035\n",
      "197 likes 0.037037037037037035\n",
      "198 x 0.037037037037037035\n",
      "199 leverage 0.037037037037037035\n",
      "200 learned 0.037037037037037035\n",
      "201 m 0.037037037037037035\n",
      "202 out 0.037037037037037035\n",
      "203 pandas 0.037037037037037035\n",
      "204 package 0.037037037037037035\n",
      "205 soon 0.037037037037037035\n",
      "206 treyhunner 0.037037037037037035\n",
      "207 some 0.037037037037037035\n",
      "208 solving 0.037037037037037035\n",
      "209 solutions 0.037037037037037035\n",
      "210 software 0.037037037037037035\n",
      "211 so 0.037037037037037035\n",
      "212 snake… 0.037037037037037035\n",
      "213 snake_case 0.037037037037037035\n",
      "214 trick 0.037037037037037035\n",
      "215 snail 0.037037037037037035\n",
      "216 smbmap 0.037037037037037035\n",
      "217 tricks 0.037037037037037035\n",
      "218 sir 0.037037037037037035\n",
      "219 shut 0.037037037037037035\n",
      "220 shit 0.037037037037037035\n",
      "221 tutorial 0.037037037037037035\n",
      "222 shared 0.037037037037037035\n",
      "223 share 0.037037037037037035\n",
      "224 sound 0.037037037037037035\n",
      "225 space 0.037037037037037035\n",
      "226 splitting 0.037037037037037035\n",
      "227 topics 0.037037037037037035\n",
      "228 test 0.037037037037037035\n",
      "229 that 0.037037037037037035\n",
      "230 theakshaya 0.037037037037037035\n",
      "231 tenho 0.037037037037037035\n",
      "232 telling 0.037037037037037035\n",
      "233 tedpetrou 0.037037037037037035\n",
      "234 technic… 0.037037037037037035\n",
      "235 teach 0.037037037037037035\n",
      "236 system 0.037037037037037035\n",
      "237 sequence 0.037037037037037035\n",
      "238 supply 0.037037037037037035\n",
      "239 there 0.037037037037037035\n",
      "240 subject 0.037037037037037035\n",
      "241 studio 0.037037037037037035\n",
      "242 these 0.037037037037037035\n",
      "243 student 0.037037037037037035\n",
      "244 string 0.037037037037037035\n",
      "245 think 0.037037037037037035\n",
      "246 started 0.037037037037037035\n",
      "247 timothykassis 0.037037037037037035\n",
      "248 them 0.037037037037037035\n",
      "249 pace 0.037037037037037035\n",
      "250 seconds 0.037037037037037035\n",
      "251 scientists 0.037037037037037035\n",
      "252 programar 0.037037037037037035\n",
      "253 programando 0.037037037037037035\n",
      "254 pradeep6kumar 0.037037037037037035\n",
      "255 practices 0.037037037037037035\n",
      "256 power 0.037037037037037035\n",
      "257 visual 0.037037037037037035\n",
      "258 por… 0.037037037037037035\n",
      "259 por 0.037037037037037035\n",
      "260 visualization 0.037037037037037035\n",
      "261 popular 0.037037037037037035\n",
      "262 vs 0.037037037037037035\n",
      "263 platform 0.037037037037037035\n",
      "264 pick 0.037037037037037035\n",
      "265 vulnerability 0.037037037037037035\n",
      "266 part 0.037037037037037035\n",
      "267 para 0.037037037037037035\n",
      "268 w/ 0.037037037037037035\n",
      "269 painter 0.037037037037037035\n",
      "270 page 0.037037037037037035\n",
      "271 visit 0.037037037037037035\n",
      "272 project 0.037037037037037035\n",
      "273 properly 0.037037037037037035\n",
      "274 protect 0.037037037037037035\n",
      "275 safe 0.037037037037037035\n",
      "276 t… 0.037037037037037035\n",
      "277 um 0.037037037037037035\n",
      "278 saberias 0.037037037037037035\n",
      "279 universities 0.037037037037037035\n",
      "280 up 0.037037037037037035\n",
      "281 urllib3 0.037037037037037035\n",
      "282 reusable 0.037037037037037035\n",
      "283 research 0.037037037037037035\n",
      "284 sculptor 0.037037037037037035\n",
      "285 reschedule 0.037037037037037035\n",
      "286 quite 0.037037037037037035\n",
      "287 questions 0.037037037037037035\n",
      "288 queria 0.037037037037037035\n",
      "289 useful… 0.037037037037037035\n",
      "290 very 0.037037037037037035\n",
      "291 pycon 0.037037037037037035\n",
      "292 pulp 0.037037037037037035\n",
      "293 video 0.037037037037037035\n",
      "294 prutor 0.037037037037037035\n",
      "295 us 0.037037037037037035\n",
      "296 see 0.037037037037037035\n",
      "297 li… 0.037037037037037035\n",
      "298 ht 0.037037037037037035\n",
      "299 bot_de_colores 0.037037037037037035\n",
      "300 book 0.037037037037037035\n",
      "301 blender 0.037037037037037035\n",
      "302 bkb-66_aa4 0.037037037037037035\n",
      "303 bigdata 0.037037037037037035\n",
      "304 big 0.037037037037037035\n",
      "305 be… 0.037037037037037035\n",
      "306 better 0.037037037037037035\n",
      "307 best 0.037037037037037035\n",
      "308 beluga 0.037037037037037035\n",
      "309 brand 0.037037037037037035\n",
      "310 been 0.037037037037037035\n",
      "311 a… 0.037037037037037035\n",
      "312 available 0.037037037037037035\n",
      "313 autom… 0.037037037037037035\n",
      "314 automation 0.037037037037037035\n",
      "315 automatetheboringstuffwithpython 0.037037037037037035\n",
      "316 at 0.037037037037037035\n",
      "317 artifi… 0.037037037037037035\n",
      "318 artificialintelligence 0.037037037037037035\n",
      "319 array 0.037037037037037035\n",
      "320 aprender 0.037037037037037035\n",
      "321 basic/int… 0.037037037037037035\n",
      "322 another 0.037037037037037035\n",
      "323 cancel 0.037037037037037035\n",
      "324 chains 0.037037037037037035\n",
      "325 decision 0.037037037037037035\n",
      "326 dataset 0.037037037037037035\n",
      "327 datascien… 0.037037037037037035\n",
      "328 data 0.037037037037037035\n",
      "329 currently 0.037037037037037035\n",
      "330 crucial 0.037037037037037035\n",
      "331 create 0.037037037037037035\n",
      "332 cozimek 0.037037037037037035\n",
      "333 courses 0.037037037037037035\n",
      "334 coronavirus 0.037037037037037035\n",
      "335 chain 0.037037037037037035\n",
      "336 consecutive 0.037037037037037035\n",
      "337 condicion 0.037037037037037035\n",
      "338 concatenate 0.037037037037037035\n",
      "339 con 0.037037037037037035\n",
      "340 computer 0.037037037037037035\n",
      "341 complex 0.037037037037037035\n",
      "342 como 0.037037037037037035\n",
      "343 coming 0.037037037037037035\n",
      "344 combine 0.037037037037037035\n",
      "345 clrit 0.037037037037037035\n",
      "346 clr 0.037037037037037035\n",
      "347 configurar 0.037037037037037035\n",
      "348 deeplearning 0.037037037037037035\n",
      "349 annodomaini 0.037037037037037035\n",
      "350 amp 0.037037037037037035\n",
      "351 //t.co/tflgutl6zd 0.037037037037037035\n",
      "352 //t.co/rfxme722m8 0.037037037037037035\n",
      "353 //t.co/r2xdjmc4pa 0.037037037037037035\n",
      "354 //t.co/k16hygmr0q 0.037037037037037035\n",
      "355 //t.co/jbuxpsm3uy 0.037037037037037035\n",
      "356 //t.co/iur9z3usyf 0.037037037037037035\n",
      "357 //t.co/hpmemrmllc 0.037037037037037035\n",
      "358 //t.co/hmgal7puhe 0.037037037037037035\n",
      "359 //t.co/hbvpiwcycn 0.037037037037037035\n",
      "360 //t.co/gvdyues6q9 0.037037037037037035\n",
      "361 //t.co/toa9gpz5sw 0.037037037037037035\n",
      "362 //t.co/gszh3edkzg 0.037037037037037035\n",
      "363 //t.co/e8224yv4on 0.037037037037037035\n",
      "364 //t.co/cc8t3curun 0.037037037037037035\n",
      "365 //t.co/c6aojalh5b 0.037037037037037035\n",
      "366 //t.co/8mkzzfzgiu 0.037037037037037035\n",
      "367 //t.co/3xepz3zwlw 0.037037037037037035\n",
      "368 //t.co/1fld0sj63w 0.037037037037037035\n",
      "369 't 0.037037037037037035\n",
      "370 're 0.037037037037037035\n",
      "371 'll 0.037037037037037035\n",
      "372 % 0.037037037037037035\n",
      "373 //t.co/gemfe8eyxc 0.037037037037037035\n",
      "374 animationnodes 0.037037037037037035\n",
      "375 //t.co/trmmonk2eh 0.037037037037037035\n",
      "376 //t.co/wizb5lm85t 0.037037037037037035\n",
      "377 amount 0.037037037037037035\n",
      "378 am 0.037037037037037035\n",
      "379 alvinfoo 0.037037037037037035\n",
      "380 alsweigart 0.037037037037037035\n",
      "381 already 0.037037037037037035\n",
      "382 alibaba 0.037037037037037035\n",
      "383 aktu_lucknow 0.037037037037037035\n",
      "384 add 0.037037037037037035\n",
      "385 accuracy 0.037037037037037035\n",
      "386 access 0.037037037037037035\n",
      "387 //t.co/uojjyf2hkl 0.037037037037037035\n",
      "388 = 0.037037037037037035\n",
      "389 6th 0.037037037037037035\n",
      "390 3 0.037037037037037035\n",
      "391 20… 0.037037037037037035\n",
      "392 2020. 0.037037037037037035\n",
      "393 2020 0.037037037037037035\n",
      "394 20 0.037037037037037035\n",
      "395 100th 0.037037037037037035\n",
      "396 100daysofcod… 0.037037037037037035\n",
      "397 100 0.037037037037037035\n",
      "398 1 0.037037037037037035\n",
      "399 96 0.037037037037037035\n",
      "400 default 0.037037037037037035\n",
      "401 because 0.037037037037037035\n",
      "402 freecodecamp 0.037037037037037035\n",
      "403 fun 0.037037037037037035\n",
      "404 from 0.037037037037037035\n",
      "405 delimiter 0.037037037037037035\n",
      "406 free 0.037037037037037035\n",
      "407 foundation 0.037037037037037035\n",
      "408 found 0.037037037037037035\n",
      "409 formula 0.037037037037037035\n",
      "410 functions 0.037037037037037035\n",
      "411 finally 0.037037037037037035\n",
      "412 filtrado 0.037037037037037035\n",
      "413 fibonacci 0.037037037037037035\n",
      "414 few 0.037037037037037035\n",
      "415 exploring 0.037037037037037035\n",
      "416 eu 0.037037037037037035\n",
      "417 engaging 0.037037037037037035\n",
      "418 em_elkan 0.037037037037037035\n",
      "419 final 0.037037037037037035\n",
      "420 em 0.037037037037037035\n",
      "421 get 0.037037037037037035\n",
      "422 gives 0.037037037037037035\n",
      "423 how 0.037037037037037035\n",
      "424 house 0.037037037037037035\n",
      "425 homeaut… 0.037037037037037035\n",
      "426 here 0.037037037037037035\n",
      "427 helping 0.037037037037037035\n",
      "428 haha 0.037037037037037035\n",
      "429 guides 0.037037037037037035\n",
      "430 github 0.037037037037037035\n",
      "431 gui 0.037037037037037035\n",
      "432 grad 0.037037037037037035\n",
      "433 gp_pulipaka 0.037037037037037035\n",
      "434 go… 0.037037037037037035\n",
      "435 got 0.037037037037037035\n",
      "436 gossithedog 0.037037037037037035\n",
      "437 google 0.037037037037037035\n",
      "438 giving 0.037037037037037035\n",
      "439 gt 0.037037037037037035\n",
      "440 effector 0.037037037037037035\n",
      "441 🤖 0.037037037037037035\n",
      "442 denial 0.037037037037037035\n",
      "443 dii_lua 0.037037037037037035\n",
      "444 dificuldade 0.037037037037037035\n",
      "445 don 0.037037037037037035\n",
      "446 down 0.037037037037037035\n",
      "447 detector 0.037037037037037035\n",
      "448 details 0.037037037037037035\n",
      "449 detect 0.037037037037037035\n",
      "450 doodle… 0.037037037037037035\n",
      "451 //t.co/f5rxgtt1ps 0.018518518518518517\n",
      "452 de 0.018518518518518517\n",
      "453 packed 0.018518518518518517\n",
      "454 //t.co/cna1tmudto 0.018518518518518517\n",
      "455 healthcare 0.018518518518518517\n",
      "456 int… 0.018518518518518517\n",
      "457 dclarediane 0.018518518518518517\n",
      "458 guy 0.018518518518518517\n",
      "459 //t.co/1jygrlehkw 0.018518518518518517\n",
      "460 //t.co/0ihvpjzfxn 0.018518518518518517\n",
      "461 high 0.018518518518518517\n",
      "462 worth 0.018518518518518517\n",
      "463 « 0.018518518518518517\n",
      "464 » 0.018518518518518517\n",
      "465 // 0.018518518518518517\n",
      "466 автор 0.018518518518518517\n",
      "467 бессмертных 0.018518518518518517\n",
      "468 выхода 0.018518518518518517\n",
      "469 и 0.018518518518518517\n",
      "470 как 0.018518518518518517\n",
      "471 можно 0.018518518518518517\n",
      "472 yutahamaguchi 0.018518518518518517\n",
      "473 itmedia 0.018518518518518517\n",
      "474 cleverlee_dope 0.018518518518518517\n",
      "475 //t.co/gzaocomwhc 0.018518518518518517\n",
      "476 vimで書いてjupyterにペーストする自己満足 0.018518518518518517\n",
      "477 karratha-tom 0.018518518518518517\n",
      "478 //t.co/sdavmrwvnp 0.018518518518518517\n",
      "479 //t.co/rmkjmb56al 0.018518518518518517\n",
      "480 passion 0.018518518518518517\n",
      "481 d.values 0.018518518518518517\n",
      "482 wa_roads 0.018518518518518517\n",
      "483 //t.co/qd5njlk0xi 0.018518518518518517\n",
      "484 //t.co/pnhsepbtbn 0.018518518518518517\n",
      "485 //t.co/pmgfft7lkg 0.018518518518518517\n",
      "486 we 0.018518518518518517\n",
      "487 //t.co/ntac26unjn 0.018518518518518517\n",
      "488 week 0.018518518518518517\n",
      "489 //t.co/mkds7abzay 0.018518518518518517\n",
      "490 were 0.018518518518518517\n",
      "491 //t.co/lxzxdjorsv 0.018518518518518517\n",
      "492 dangerous 0.018518518518518517\n",
      "493 whitehouse 0.018518518518518517\n",
      "494 graphical 0.018518518518518517\n",
      "495 gro… 0.018518518518518517\n",
      "496 //t.co/imtkgylsxh 0.018518518518518517\n",
      "497 desired 0.018518518518518517\n",
      "498 ondcp 0.018518518518518517\n",
      "499 itmedia_news 0.018518518518518517\n",
      "500 момента 0.018518518518518517\n",
      "501 м… 0.018518518518518517\n",
      "502 паразитов 0.018518518518518517\n",
      "503 日本ディープラーニング協会、機械学習やpythonを学べる講座を無料公開 0.018518518518518517\n",
      "504 日本ディープラーニング協会、機械学習やpythonを学べる講座を無料配信 0.018518518518518517\n",
      "505 知り合いの小学生が 0.018518518518518517\n",
      "506 突っ込みどころいっぱいですが、応… 0.018518518518518517\n",
      "507 競技プログラミングすげー勧められたけど始めたところでやれるんだろうか。やるならpythonだろうけど 0.018518518518518517\n",
      "508 自分も、cb中にus500をロング注文してみたところ、注文が通って保有した状態だったのですが、22:30再びチャートが動き出したところで、利確しようと思ったら、できなかったです！ 0.018518518518518517\n",
      "509 言い換えると、c… 0.018518518518518517\n",
      "510 辞書型のvalues 0.018518518518518517\n",
      "511 駆け出しエンジニアと繋がりたい 0.018518518518518517\n",
      "512 구성하시는걸 0.018518518518518517\n",
      "513 또는 0.018518518518518517\n",
      "514 로 0.018518518518518517\n",
      "515 서버로 0.018518518518518517\n",
      "516 수 0.018518518518518517\n",
      "517 예시로는 0.018518518518518517\n",
      "518 있습니다 0.018518518518518517\n",
      "519 작성하시는게 0.018518518518518517\n",
      "520 추천드리고 0.018518518518518517\n",
      "521 편하실 0.018518518518518517\n",
      "522 's 0.018518518518518517\n",
      "523 🐕 0.018518518518518517\n",
      "524 dea 0.018518518518518517\n",
      "525 order 0.018518518518518517\n",
      "526 only 0.018518518518518517\n",
      "527 😂 0.018518518518518517\n",
      "528 外出控える人に「学ぶきっかけを」 0.018518518518518517\n",
      "529 php 0.018518518518518517\n",
      "530 今日は少し早いよっ🌻 0.018518518518518517\n",
      "531 メソッドは中に登録されている値全てをリストとして返すので、これを利用してif 0.018518518518518517\n",
      "532 полундра-хуюндра 0.018518518518518517\n",
      "533 прошло 0.018518518518518517\n",
      "534 с 0.018518518518518517\n",
      "535 суток 0.018518518518518517\n",
      "536 таких 0.018518518518518517\n",
      "537 трое 0.018518518518518517\n",
      "538 хитов 0.018518518518518517\n",
      "539 юмористических 0.018518518518518517\n",
      "540 ... 0.018518518518518517\n",
      "541 ) 0.018518518518518517\n",
      "542 ( 0.018518518518518517\n",
      "543 included 0.018518518518518517\n",
      "544 「私は会社をどうやっていくのがいいか、まだ知っていることが少ないので、学びながらやってみようと思っています。pythonも、まだやりはじめたばかり」 0.018518518518518517\n",
      "545 」 0.018518518518518517\n",
      "546 このあと19時から「不動産価格データを使って遊ぼう 0.018518518518518517\n",
      "547 たしかどこかの大学もpythonが無料で学べた気がする 0.018518518518518517\n",
      "548 たちまちメモ。 0.018518518518518517\n",
      "549 などとして条件分岐などができる。 0.018518518518518517\n",
      "550 に会社作りました。 0.018518518518518517\n",
      "551 はぁ…目まっ黒のボールパイソンは 0.018518518518518517\n",
      "552 はじめましてっ🥰 0.018518518518518517\n",
      "553 もれなく可愛い…………………………… 0.018518518518518517\n",
      "554 らきすた２２話、たまに観たくなる。観てる。 0.018518518518518517\n",
      "555 エストニア 0.018518518518518517\n",
      "556 プログラミング初心者 0.018518518518518517\n",
      "557 今まで自鯖で動かしていた何某かの指標を取ってきてどこかしらに送る系のスクリプト、昔に作ったphpで正規表現頑張ってスクレイピングしてくるみたいなやつ多かったけど、一通りpythonにリプレイスしたった。 0.018518518518518517\n",
      "558 victory 0.018518518518518517\n",
      "559 //t.co/wbcldzbjrg 0.018518518518518517\n",
      "560 //t.co/vmnoipvqkv 0.018518518518518517\n",
      "561 entiendo 0.018518518518518517\n",
      "562 epsilontheory 0.018518518518518517\n",
      "563 most 0.018518518518518517\n",
      "564 beyond 0.018518518518518517\n",
      "565 er 0.018518518518518517\n",
      "566 monty 0.018518518518518517\n",
      "567 significant 0.018518518518518517\n",
      "568 bellaasays2 0.018518518518518517\n",
      "569 begins 0.018518518518518517\n",
      "570 sketch 0.018518518518518517\n",
      "571 slammed 0.018518518518518517\n",
      "572 seen 0.018518518518518517\n",
      "573 ever 0.018518518518518517\n",
      "574 mistake 0.018518518518518517\n",
      "575 bands 0.018518518518518517\n",
      "576 michaeljohns 0.018518518518518517\n",
      "577 mexico 0.018518518518518517\n",
      "578 mexican 0.018518518518518517\n",
      "579 executive 0.018518518518518517\n",
      "580 memphis 0.018518518518518517\n",
      "581 members 0.018518518518518517\n",
      "582 common 0.018518518518518517\n",
      "583 extreme 0.018518518518518517\n",
      "584 arrest 0.018518518518518517\n",
      "585 ibella 0.018518518518518517\n",
      "586 favourite 0.018518518518518517\n",
      "587 news 0.018518518518518517\n",
      "588 coding 0.018518518518518517\n",
      "589 clearance 0.018518518518518517\n",
      "590 python_btc_bot 0.018518518518518517\n",
      "591 pythonでstr.lengthって書いて怒られるのはよくやります 0.018518518518518517\n",
      "592 pythonでデータ分析をしちゃうんです🐍✨✨ 0.018518518518518517\n",
      "593 pytho… 0.018518518518518517\n",
      "594 checking 0.018518518518518517\n",
      "595 dthompsondev 0.018518518518518517\n",
      "596 m… 0.018518518518518517\n",
      "597 rattlesnake 0.018518518518518517\n",
      "598 rd 0.018518518518518517\n",
      "599 caution 0.018518518518518517\n",
      "600 cold 0.018518518518518517\n",
      "601 realdonaldtrump 0.018518518518518517\n",
      "602 cartel 0.018518518518518517\n",
      "603 captainmeowdy 0.018518518518518517\n",
      "604 restful한 0.018518518518518517\n",
      "605 easy 0.018518518518518517\n",
      "606 call 0.018518518518518517\n",
      "607 c5dingb5y 0.018518518518518517\n",
      "608 by 0.018518518518518517\n",
      "609 drug 0.018518518518518517\n",
      "610 schoo_inc 0.018518518518518517\n",
      "611 multiple 0.018518518518518517\n",
      "612 scored 0.018518518518518517\n",
      "613 caught 0.018518518518518517\n",
      "614 via 0.018518518518518517\n",
      "615 stage 0.018518518518518517\n",
      "616 statement 0.018518518518518517\n",
      "617 600 0.018518518518518517\n",
      "618 time😎 0.018518518518518517\n",
      "619 50rt 0.018518518518518517\n",
      "620 tmo_tfb 0.018518518518518517\n",
      "621 4wds 0.018518518518518517\n",
      "622 together 0.018518518518518517\n",
      "623 trees 0.018518518518518517\n",
      "624 np_ur_ 0.018518518518518517\n",
      "625 nyc 0.018518518518518517\n",
      "626 pool 0.018518518518518517\n",
      "627 trucks 0.018518518518518517\n",
      "628 discordのbotはnodeよりpythonの方が日本語の記事多そうなんだよな 0.018518518518518517\n",
      "629 trump 0.018518518518518517\n",
      "630 twinygoals 0.018518518518518517\n",
      "631 typescript 0.018518518518518517\n",
      "632 play 0.018518518518518517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633 detour 0.018518518518518517\n",
      "634 leads 0.018518518518518517\n",
      "635 //… 0.018518518518518517\n",
      "636 //t… 0.018518518518518517\n",
      "637 //t.co/xd0tybjrof 0.018518518518518517\n",
      "638 goal 0.018518518518518517\n",
      "639 usgslandsat 0.018518518518518517\n",
      "640 nada 0.018518518518518517\n",
      "641 2017 0.018518518518518517\n",
      "642 are 0.018518518518518517\n",
      "643 😊👍 0.018518518518518517\n",
      "644 notepad+… 0.018518518518518517\n",
      "645 stone 0.018518518518518517\n",
      "646 store 0.018518518518518517\n",
      "647 aoixx201 0.018518518518518517\n",
      "648 anything 0.018518518518518517\n",
      "649 ff 0.018518518518518517\n",
      "650 files.… 0.018518518518518517\n",
      "651 process 0.018518518518518517\n",
      "652 proceduralart 0.018518518518518517\n",
      "653 americafirst150 0.018518518518518517\n",
      "654 tech 0.018518518518518517\n",
      "655 price 0.018518518518518517\n",
      "656 they 0.018518518518518517\n",
      "657 fire 0.018518518518518517\n",
      "658 following 0.018518518518518517\n",
      "659 doj… 0.018518518518518517\n",
      "660 alburov 0.018518518518518517\n",
      "661 texas 0.018518518518518517\n",
      "662 l… 0.018518518518518517\n",
      "663 against 0.018518518518518517\n",
      "664 admin 0.018518518518518517\n",
      "665 loving 0.018518518518518517\n",
      "666 dog 0.018518518518518517\n",
      "667 then 0.018518518518518517\n",
      "668 located 0.018518518518518517\n",
      "669 folks 0.018518518518518517\n",
      "670 open 0.018518518518518517\n"
     ]
    }
   ],
   "source": [
    "# Printing the names of the top features\n",
    "for i, feature_index in enumerate(top_features):\n",
    "    print(i, dv.feature_names_[feature_index], np.exp(feature_probabilities[1][feature_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\rmart\\\\OneDrive\\\\Desktop\\\\Pace\\\\CS619\\\\Chapter07\\\\Data\\\\python_context.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exporting model file to use in Chapter 07 assignment\n",
    "import joblib\n",
    "output_filename = os.path.join(os.path.expanduser(\"~\"), 'OneDrive', 'Desktop', 'Pace', 'CS619', 'Chapter07', 'Data', 'python_context.pkl')\n",
    "joblib.dump(model, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
