{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program used to recreate dataset\n",
    "\n",
    "import os\n",
    "\n",
    "tweet_filename = os.path.join(os.path.expanduser('~'), 'OneDrive', 'Desktop', 'Pace', 'CS619', 'Chapter06', 'Data', 'replicable_python_tweets.json')\n",
    "labels_filename = os.path.join(os.path.expanduser('~'), 'OneDrive', 'Desktop', 'Pace', 'CS619', 'Chapter06', 'Data', 'replicable_python_classes.json')\n",
    "replicable_dataset = os.path.join(os.path.expanduser('~'), 'OneDrive', 'Desktop', 'Pace', 'CS619', 'Chapter06', 'Data', 'replicable_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the tweet IDs from the file\n",
    "import json\n",
    "\n",
    "with open(replicable_dataset) as inf:\n",
    "    tweet_ids = json.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list to store the labels for tweets that we actually recover from twitter and create a dictionary mapping the IDs to labels\n",
    "actual_labels = []\n",
    "label_mapping = dict(tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a twitter server to collect all tweets\n",
    "import twitter\n",
    "\n",
    "consumer_key = 'ysled1Vd292M3P6Y5HBhdInrT'\n",
    "consumer_secret = 'icsVBE8MASwZNrdRYBlr74Rx955rtgaBVbZOWuoPDgvOFKbDvC'\n",
    "access_token = '1223330048-VDr9S3dsWm8b3l8EBDNK1DwmXD8V0hzaLKmr1gn'\n",
    "access_token_secret = 'nEnLTbYQ7pfr5EqzM7BLNNcAk3RmT3gZUzvuUXskUECUh'\n",
    "authorization = twitter.OAuth(access_token, access_token_secret, consumer_key, consumer_secret)\n",
    "t = twitter.Twitter(auth=authorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through each of the tweet ids and get the information for each of them\n",
    "all_ids = [tweet_id for tweet_id, label in tweet_ids]\n",
    "\n",
    "with open(tweet_filename, 'a') as output_file:\n",
    "    # We can lookup 100 tweets at a time, which saves time in asking twitter for them\n",
    "    for start_index in range(0, len(all_ids), 100):\n",
    "        id_string = ','.join(str(i) for i in all_ids[start_index:start_index+100])\n",
    "        search_results = t.statuses.lookup(_id=id_string)\n",
    "        for tweet in search_results:\n",
    "            if 'text' in tweet:\n",
    "                # Valid tweet - save to file\n",
    "                output_file.write(json.dumps(tweet))\n",
    "                output_file.write('\\n\\n')\n",
    "                actual_labels.append(label_mapping[tweet['id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we check each tweet to see if it is a valid tweet and then save it to our file if it is\n",
    "with open(labels_filename, 'w') as outf:\n",
    "    json.dump(actual_labels, outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
